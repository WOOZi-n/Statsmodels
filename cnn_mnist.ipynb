{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWHPxR+bvx46vZUpagAnLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WOOZi-n/Statsmodels/blob/main/cnn_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "rU9WtME0zGB3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra6mVXhY0s1-",
        "outputId": "59309fd9-22f9-4bbd-f6a7-080fd120de2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jU9aTUXlyTaI"
      },
      "outputs": [],
      "source": [
        "# 기본 cnn 구현\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(64, 7, activation = 'relu', padding = 'same', input_shape = [28,28,1]),\n",
        "    keras.layers.MaxPool2D(2),\n",
        "    keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same'),\n",
        "    keras.layers.MaxPool2D(2),\n",
        "    keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same'),\n",
        "    keras.layers.GlobalAveragePooling2D(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation = 'relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(64, activation = 'relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation = 'softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = keras.optimizers.Nadam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "7VWsJFew0ZwV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwTfNbUv0oLi",
        "outputId": "86bf5afa-848b-4f45-baea-ffc4ecb36d97"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 330s 175ms/step - loss: 0.1759 - accuracy: 0.9553\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 322s 172ms/step - loss: 0.0974 - accuracy: 0.9780\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 359s 191ms/step - loss: 0.0779 - accuracy: 0.9818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test) # 정확도 98.7퍼센트"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEE6LGlm7TuT",
        "outputId": "51e2270d-2507-46d3-8cee-d983d3783a02"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 13s 41ms/step - loss: 0.0437 - accuracy: 0.9869\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04373524338006973, 0.9868999719619751]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lenet_5 구현\n",
        "\n",
        "lenet_5 = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(6, 5, padding = 'same', activation  = 'tanh', input_shape = [28,28,1]),\n",
        "    keras.layers.AvgPool2D(2),\n",
        "    keras.layers.Conv2D(16, 5, padding = 'valid',activation = 'tanh'),\n",
        "    keras.layers.AvgPool2D(2),\n",
        "    keras.layers.Conv2D(120,5, padding = 'valid', activation  ='tanh'),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(84, activation  ='tanh'),\n",
        "    keras.layers.Dense(10, activation = 'softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "0dUPSs8I1L59"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_5.compile(optimizer= keras.optimizers.Nadam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "history = lenet_5.fit(X_train, y_train, epochs =3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46NRmMUe2s9P",
        "outputId": "48361600-a9bd-471f-b725-72ea2580bb4f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 42s 22ms/step - loss: 0.1556 - accuracy: 0.9544\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 42s 22ms/step - loss: 0.0660 - accuracy: 0.9799\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 40s 22ms/step - loss: 0.0515 - accuracy: 0.9841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_5.evaluate(X_test, y_test) #정확도 98.4퍼센트"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3UTsULF8gSQ",
        "outputId": "d78180bd-5842-40a3-d344-f432411efba4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0476 - accuracy: 0.9839\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0476493202149868, 0.9839000105857849]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alexnet\n",
        "\n",
        "alexnet = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, 7, activation = 'relu',padding = 'valid', input_shape = [28,28,1]),\n",
        "    keras.layers.Lambda(lambda x : tf.nn.local_response_normalization(x)),\n",
        "    keras.layers.MaxPool2D(2),\n",
        "    keras.layers.Conv2D(64, 3, activation  = 'relu', padding = 'same'),\n",
        "    keras.layers.Lambda(lambda x : tf.nn.local_response_normalization(x)),\n",
        "    keras.layers.MaxPool2D(2),\n",
        "    keras.layers.Conv2D(128,3,activation  ='relu', padding = 'same'),\n",
        "    keras.layers.Conv2D(256,3,activation  ='relu', padding = 'same'),\n",
        "    keras.layers.MaxPool2D(2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(84, activation  ='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation  = 'softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "GsWm2MvS3rWL"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet.compile(optimizer = keras.optimizers.Nadam(), loss = 'sparse_categorical_crossentropy', metrics= ['accuracy'])"
      ],
      "metadata": {
        "id": "McBCo_026_dO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = alexnet.fit(X_train, y_train, epochs = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szJ7RPyc9HJq",
        "outputId": "4c55eee2-8002-421e-9c86-a2eed3b17aed"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 187s 99ms/step - loss: 0.1848 - accuracy: 0.9469\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 185s 99ms/step - loss: 0.0664 - accuracy: 0.9824\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 183s 98ms/step - loss: 0.0518 - accuracy: 0.9863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet.evaluate(X_test, y_test) # 정확도 99퍼"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YX9FaIi_q88",
        "outputId": "e1df3e6f-5c6f-4e7c-9093-efc2514eb4e5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 8s 25ms/step - loss: 0.0334 - accuracy: 0.9895\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.033438194543123245, 0.9894999861717224]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GoogLeNEt의 인셉션 모듈 구현\n",
        "input = np.arange(784).reshape(1,28,28,1)\n",
        "input = tf.constant(input, dtype = tf.float32)\n",
        "\n",
        "conv1 = keras.layers.Conv2D(64, 1, activation  = 'relu')(input)\n",
        "\n",
        "conv2_1 = keras.layers.Conv2D(64,1, activation = 'relu')(input)\n",
        "conv2_2 = keras.layers.Conv2D(128,3,padding = 'same', activation = 'relu')(conv2_1)\n",
        "\n",
        "conv3_1 = keras.layers.Conv2D(64,1, activation = 'relu')(input)\n",
        "conv3_2 = keras.layers.Conv2D(128,5, padding = 'same', activation = 'relu')(conv3_1)\n",
        "\n",
        "pool = keras.layers.MaxPool2D(3, strides = 1, padding = 'same')(input)\n",
        "conv4 = keras.layers.Conv2D(64, 1, activation = 'relu')(pool)\n",
        "\n",
        "concat = keras.layers.Concatenate(axis =3)([conv1, conv2_2, conv3_2, conv4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XKQzcyyL9kWi",
        "outputId": "7719c60c-49d7-4f62-d15d-84a7705d68d5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-3ffcbe4d10fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv3_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0minception\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mis_input_keras_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \"\"\"\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: [[[[  0.]\n   [  1.]\n   [  2.]\n   [  3.]\n   [  4.]\n   [  5.]\n   [  6.]\n   [  7.]\n   [  8.]\n   [  9.]\n   [ 10.]\n   [ 11.]\n   [ 12.]\n   [ 13.]\n   [ 14.]\n   [ 15.]\n   [ 16.]\n   [ 17.]\n   [ 18.]\n   [ 19.]\n   [ 20.]\n   [ 21.]\n   [ 22.]\n   [ 23.]\n   [ 24.]\n   [ 25.]\n   [ 26.]\n   [ 27.]]\n\n  [[ 28.]\n   [ 29.]\n   [ 30.]\n   [ 31.]\n   [ 32.]\n   [ 33.]\n   [ 34.]\n   [ 35.]\n   [ 36.]\n   [ 37.]\n   [ 38.]\n   [ 39.]\n   [ 40.]\n   [ 41.]\n   [ 42.]\n   [ 43.]\n   [ 44.]\n   [ 45.]\n   [ 46.]\n   [ 47.]\n   [ 48.]\n   [ 49.]\n   [ 50.]\n   [ 51.]\n   [ 52.]\n   [ 53.]\n   [ 54.]\n   [ 55.]]\n\n  [[ 56.]\n   [ 57.]\n   [ 58.]\n   [ 59.]\n   [ 60.]\n   [ 61.]\n   [ 62.]\n   [ 63.]\n   [ 64.]\n   [ 65.]\n   [ 66.]\n   [ 67.]\n   [ 68.]\n   [ 69.]\n   [ 70.]\n   [ 71.]\n   [ 72.]\n   [ 73.]\n   [ 74.]\n   [ 75.]\n   [ 76.]\n   [ 77.]\n   [ 78.]\n   [ 79.]\n   [ 80.]\n   [ 81.]\n   [ 82.]\n   [ 83.]]\n\n  [[ 84.]\n   [ 85.]\n   [ 86.]\n   [ 87.]\n   [ 88.]\n   [ 89.]\n   [ 90.]\n   [ 91.]\n   [ 92.]\n   [ 93.]\n   [ 94.]\n   [ 95.]\n   [ 96.]\n   [ 97.]\n   [ 98.]\n   [ 99.]\n   [100.]\n   [101.]\n   [102.]\n   [103.]\n   [104.]\n   [105.]\n   [106.]\n   [107.]\n   [108.]\n   [109.]\n   [110.]\n   [111.]]\n\n  [[112.]\n   [113.]\n   [114.]\n   [115.]\n   [116.]\n   [117.]\n   [118.]\n   [119.]\n   [120.]\n   [121.]\n   [122.]\n   [123.]\n   [124.]\n   [125.]\n   [126.]\n   [127.]\n   [128.]\n   [129.]\n   [130.]\n   [131.]\n   [132.]\n   [133.]\n   [134.]\n   [135.]\n   [136.]\n   [137.]\n   [138.]\n   [139.]]\n\n  [[140.]\n   [141.]\n   [142.]\n   [143.]\n   [144.]\n   [145.]\n   [146.]\n   [147.]\n   [148.]\n   [149.]\n   [150.]\n   [151.]\n   [152.]\n   [153.]\n   [154.]\n   [155.]\n   [156.]\n   [157.]\n   [158.]\n   [159.]\n   [160.]\n   [161.]\n   [162.]\n   [163.]\n   [164.]\n   [165.]\n   [166.]\n   [167.]]\n\n  [[168.]\n   [169.]\n   [170.]\n   [171.]\n   [172.]\n   [173.]\n   [174.]\n   [175.]\n   [176.]\n   [177.]\n   [178.]\n   [179.]\n   [180.]\n   [181.]\n   [182.]\n   [183.]\n   [184.]\n   [185.]\n   [186.]\n   [187.]\n   [188.]\n   [189.]\n   [190.]\n   [191.]\n   [192.]\n   [193.]\n   [194.]\n   [195.]]\n\n  [[196.]\n   [197.]\n   [198.]\n   [199.]\n   [200.]\n   [201.]\n   [202.]\n   [203.]\n   [204.]\n   [205.]\n   [206.]\n   [207.]\n   [208.]\n   [209.]\n   [210.]\n   [211.]\n   [212.]\n   [213.]\n   [214.]\n   [215.]\n   [216.]\n   [217.]\n   [218.]\n   [219.]\n   [220.]\n   [221.]\n   [222.]\n   [223.]]\n\n  [[224.]\n   [225.]\n   [226.]\n   [227.]\n   [228.]\n   [229.]\n   [230.]\n   [231.]\n   [232.]\n   [233.]\n   [234.]\n   [235.]\n   [236.]\n   [237.]\n   [238.]\n   [239.]\n   [240.]\n   [241.]\n   [242.]\n   [243.]\n   [244.]\n   [245.]\n   [246.]\n   [247.]\n   [248.]\n   [249.]\n   [250.]\n   [251.]]\n\n  [[252.]\n   [253.]\n   [254.]\n   [255.]\n   [256.]\n   [257.]\n   [258.]\n   [259.]\n   [260.]\n   [261.]\n   [262.]\n   [263.]\n   [264.]\n   [265.]\n   [266.]\n   [267.]\n   [268.]\n   [269.]\n   [270.]\n   [271.]\n   [272.]\n   [273.]\n   [274.]\n   [275.]\n   [276.]\n   [277.]\n   [278.]\n   [279.]]\n\n  [[280.]\n   [281.]\n   [282.]\n   [283.]\n   [284.]\n   [285.]\n   [286.]\n   [287.]\n   [288.]\n   [289.]\n   [290.]\n   [291.]\n   [292.]\n   [293.]\n   [294.]\n   [295.]\n   [296.]\n   [297.]\n   [298.]\n   [299.]\n   [300.]\n   [301.]\n   [302.]\n   [303.]\n   [304.]\n   [305.]\n   [306.]\n   [307.]]\n\n  [[308.]\n   [309.]\n   [310.]\n   [311.]\n   [312.]\n   [313.]\n   [314.]\n   [315.]\n   [316.]\n   [317.]\n   [318.]\n   [319.]\n   [320.]\n   [321.]\n   [322.]\n   [323.]\n   [324.]\n   [325.]\n   [326.]\n   [327.]\n   [328.]\n   [329.]\n   [330.]\n   [331.]\n   [332.]\n   [333.]\n   [334.]\n   [335.]]\n\n  [[336.]\n   [337.]\n   [338.]\n   [339.]\n   [340.]\n   [341.]\n   [342.]\n   [343.]\n   [344.]\n   [345.]\n   [346.]\n   [347.]\n   [348.]\n   [349.]\n   [350.]\n   [351.]\n   [352.]\n   [353.]\n   [354.]\n   [355.]\n   [356.]\n   [357.]\n   [358.]\n   [359.]\n   [360.]\n   [361.]\n   [362.]\n   [363.]]\n\n  [[364.]\n   [365.]\n   [366.]\n   [367.]\n   [368.]\n   [369.]\n   [370.]\n   [371.]\n   [372.]\n   [373.]\n   [374.]\n   [375.]\n   [376.]\n   [377.]\n   [378.]\n   [379.]\n   [380.]\n   [381.]\n   [382.]\n   [383.]\n   [384.]\n   [385.]\n   [386.]\n   [387.]\n   [388.]\n   [389.]\n   [390.]\n   [391.]]\n\n  [[392.]\n   [393.]\n   [394.]\n   [395.]\n   [396.]\n   [397.]\n   [398.]\n   [399.]\n   [400.]\n   [401.]\n   [402.]\n   [403.]\n   [404.]\n   [405.]\n   [406.]\n   [407.]\n   [408.]\n   [409.]\n   [410.]\n   [411.]\n   [412.]\n   [413.]\n   [414.]\n   [415.]\n   [416.]\n   [417.]\n   [418.]\n   [419.]]\n\n  [[420.]\n   [421.]\n   [422.]\n   [423.]\n   [424.]\n   [425.]\n   [426.]\n   [427.]\n   [428.]\n   [429.]\n   [430.]\n   [431.]\n   [432.]\n   [433.]\n   [434.]\n   [435.]\n   [436.]\n   [437.]\n   [438.]\n   [439.]\n   [440.]\n   [441.]\n   [442.]\n   [443.]\n   [444.]\n   [445.]\n   [446.]\n   [447.]]\n\n  [[448.]\n   [449.]\n   [450.]\n   [451.]\n   [452.]\n   [453.]\n   [454.]\n   [455.]\n   [456.]\n   [457.]\n   [458.]\n   [459.]\n   [460.]\n   [461.]\n   [462.]\n   [463.]\n   [464.]\n   [465.]\n   [466.]\n   [467.]\n   [468.]\n   [469.]\n   [470.]\n   [471.]\n   [472.]\n   [473.]\n   [474.]\n   [475.]]\n\n  [[476.]\n   [477.]\n   [478.]\n   [479.]\n   [480.]\n   [481.]\n   [482.]\n   [483.]\n   [484.]\n   [485.]\n   [486.]\n   [487.]\n   [488.]\n   [489.]\n   [490.]\n   [491.]\n   [492.]\n   [493.]\n   [494.]\n   [495.]\n   [496.]\n   [497.]\n   [498.]\n   [499.]\n   [500.]\n   [501.]\n   [502.]\n   [503.]]\n\n  [[504.]\n   [505.]\n   [506.]\n   [507.]\n   [508.]\n   [509.]\n   [510.]\n   [511.]\n   [512.]\n   [513.]\n   [514.]\n   [515.]\n   [516.]\n   [517.]\n   [518.]\n   [519.]\n   [520.]\n   [521.]\n   [522.]\n   [523.]\n   [524.]\n   [525.]\n   [526.]\n   [527.]\n   [528.]\n   [529.]\n   [530.]\n   [531.]]\n\n  [[532.]\n   [533.]\n   [534.]\n   [535.]\n   [536.]\n   [537.]\n   [538.]\n   [539.]\n   [540.]\n   [541.]\n   [542.]\n   [543.]\n   [544.]\n   [545.]\n   [546.]\n   [547.]\n   [548.]\n   [549.]\n   [550.]\n   [551.]\n   [552.]\n   [553.]\n   [554.]\n   [555.]\n   [556.]\n   [557.]\n   [558.]\n   [559.]]\n\n  [[560.]\n   [561.]\n   [562.]\n   [563.]\n   [564.]\n   [565.]\n   [566.]\n   [567.]\n   [568.]\n   [569.]\n   [570.]\n   [571.]\n   [572.]\n   [573.]\n   [574.]\n   [575.]\n   [576.]\n   [577.]\n   [578.]\n   [579.]\n   [580.]\n   [581.]\n   [582.]\n   [583.]\n   [584.]\n   [585.]\n   [586.]\n   [587.]]\n\n  [[588.]\n   [589.]\n   [590.]\n   [591.]\n   [592.]\n   [593.]\n   [594.]\n   [595.]\n   [596.]\n   [597.]\n   [598.]\n   [599.]\n   [600.]\n   [601.]\n   [602.]\n   [603.]\n   [604.]\n   [605.]\n   [606.]\n   [607.]\n   [608.]\n   [609.]\n   [610.]\n   [611.]\n   [612.]\n   [613.]\n   [614.]\n   [615.]]\n\n  [[616.]\n   [617.]\n   [618.]\n   [619.]\n   [620.]\n   [621.]\n   [622.]\n   [623.]\n   [624.]\n   [625.]\n   [626.]\n   [627.]\n   [628.]\n   [629.]\n   [630.]\n   [631.]\n   [632.]\n   [633.]\n   [634.]\n   [635.]\n   [636.]\n   [637.]\n   [638.]\n   [639.]\n   [640.]\n   [641.]\n   [642.]\n   [643.]]\n\n  [[644.]\n   [645.]\n   [646.]\n   [647.]\n   [648.]\n   [649.]\n   [650.]\n   [651.]\n   [652.]\n   [653.]\n   [654.]\n   [655.]\n   [656.]\n   [657.]\n   [658.]\n   [659.]\n   [660.]\n   [661.]\n   [662.]\n   [663.]\n   [664.]\n   [665.]\n   [666.]\n   [667.]\n   [668.]\n   [669.]\n   [670.]\n   [671.]]\n\n  [[672.]\n   [673.]\n   [674.]\n   [675.]\n   [676.]\n   [677.]\n   [678.]\n   [679.]\n   [680.]\n   [681.]\n   [682.]\n   [683.]\n   [684.]\n   [685.]\n   [686.]\n   [687.]\n   [688.]\n   [689.]\n   [690.]\n   [691.]\n   [692.]\n   [693.]\n   [694.]\n   [695.]\n   [696.]\n   [697.]\n   [698.]\n   [699.]]\n\n  [[700.]\n   [701.]\n   [702.]\n   [703.]\n   [704.]\n   [705.]\n   [706.]\n   [707.]\n   [708.]\n   [709.]\n   [710.]\n   [711.]\n   [712.]\n   [713.]\n   [714.]\n   [715.]\n   [716.]\n   [717.]\n   [718.]\n   [719.]\n   [720.]\n   [721.]\n   [722.]\n   [723.]\n   [724.]\n   [725.]\n   [726.]\n   [727.]]\n\n  [[728.]\n   [729.]\n   [730.]\n   [731.]\n   [732.]\n   [733.]\n   [734.]\n   [735.]\n   [736.]\n   [737.]\n   [738.]\n   [739.]\n   [740.]\n   [741.]\n   [742.]\n   [743.]\n   [744.]\n   [745.]\n   [746.]\n   [747.]\n   [748.]\n   [749.]\n   [750.]\n   [751.]\n   [752.]\n   [753.]\n   [754.]\n   [755.]]\n\n  [[756.]\n   [757.]\n   [758.]\n   [759.]\n   [760.]\n   [761.]\n   [762.]\n   [763.]\n   [764.]\n   [765.]\n   [766.]\n   [767.]\n   [768.]\n   [769.]\n   [770.]\n   [771.]\n   [772.]\n   [773.]\n   [774.]\n   [775.]\n   [776.]\n   [777.]\n   [778.]\n   [779.]\n   [780.]\n   [781.]\n   [782.]\n   [783.]]]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBzz8NET_73X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}